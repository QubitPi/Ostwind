[{"categories":null,"contents":"Buckets A bucket is a container for objects stored in Amazon S3. Every object is contained in a bucket. For example, if the object named photos/puppy.jpg is stored in the awsexamplebucket1 bucket in the US West (Oregon) Region, then it is addressable using the URL https://awsexamplebucket1.s3.us-west-2.amazonaws.com/photos/puppy.jpg.\nBuckets serve several purposes:\n They organize the Amazon S3 namespace at the highest level. They identify the account responsible for storage and data transfer charges. They play a role in access control. They serve as the unit of aggregation for usage reporting.  You can configure buckets so that they are created in a specific AWS Region. For more information, see Accessing a Bucket. You can also configure a bucket so that every time an object is added to it, Amazon S3 generates a unique version ID and assigns it to the object. For more information, see Using Versioning.\nFor more information about buckets, see Working with Amazon S3 Buckets.\nObjects Objects are the fundamental entities stored in Amazon S3. Objects consist of object data and metadata. The data portion is opaque to Amazon S3. The metadata is a set of name-value pairs that describe the object. These include some default metadata, such as the date last modified, and standard HTTP metadata, such as Content-Type. You can also specify custom metadata at the time the object is stored.\nAn object is uniquely identified within a bucket by a key (name) and a version ID. For more information, see Keys and Using Versioning.\nKeys A key is the unique identifier for an object within a bucket. Every object in a bucket has exactly one key. The combination of a bucket, key, and version ID uniquely identify each object. So you can think of Amazon S3 as a basic data map between \u0026ldquo;bucket + key + version\u0026rdquo; and the object itself. Every object in Amazon S3 can be uniquely addressed through the combination of the web service endpoint, bucket name, key, and optionally, a version. For example, in the URL https://doc.s3.amazonaws.com/2006-03-01/AmazonS3.wsdl, \u0026ldquo;doc\u0026rdquo; is the name of the bucket and \u0026ldquo;2006-03-01/AmazonS3.wsdl\u0026rdquo; is the key.\nFor more information about object keys, see Object Keys.\nRegions You can choose the geographical AWS Region where Amazon S3 will store the buckets that you create. You might choose a Region to optimize latency, minimize costs, or address regulatory requirements. Objects stored in a Region never leave the Region unless you explicitly transfer them to another Region. For example, objects stored in the Europe (Ireland) Region never leave it.\nAmazon S3 Data Consistency Model Amazon S3 provides read-after-write consistency for PUTS of new objects in your S3 bucket in all Regions with one caveat. The caveat is that if you make a HEAD or GET request to a key name before the object is created, then create the object shortly after that, a subsequent GET might not return the object due to eventual consistency.\nAmazon S3 offers eventual consistency for overwrite PUTS and DELETES in all Regions.\nUpdates to a single key are atomic. For example, if you PUT to an existing key, a subsequent read might return the old data or the updated data, but it never returns corrupted or partial data.\nAmazon S3 achieves high availability by replicating data across multiple servers within AWS data centers. If a PUT request is successful, your data is safely stored. However, information about the changes must replicate across Amazon S3, which can take some time, and so you might observe the following behaviors:\n A process writes a new object to Amazon S3 and immediately lists keys within its bucket. Until the change is fully propagated, the object might not appear in the list. A process replaces an existing object and immediately tries to read it. Until the change is fully propagated, Amazon S3 might return the previous data. A process deletes an existing object and immediately tries to read it. Until the deletion is fully propagated, Amazon S3 might return the deleted data. A process deletes an existing object and immediately lists keys within its bucket. Until the deletion is fully propagated, Amazon S3 might list the deleted object.  Updates are key-based. There is no way to make atomic updates across keys. For example, you cannot make the update of one key dependent on the update of another key unless you design this functionality into your application.\nBuckets have a similar consistency model, with the same caveats. For example, if you delete a bucket and immediately list all buckets, Amazon S3 might still appear in the list.\nThe following table describes the characteristics of an eventually consistent read and a consistent read:\n   Eventually consistent read Consistent read     Stale reads possible No stale reads   Lowest read latency Potential higher read latency   Highest read throughput Potential lower read throughput    Concurrent applications In this example, both W1 (write 1) and W2 (write 2) complete before the start of R1 (read 1) and R2 (read 2). For a consistent read, R1 and R2 both return color = ruby. For an eventually consistent read, R1 and R2 might return color = red or color = ruby depending on the amount of time that has elapsed.\nIn the next example, W2 does not complete before the start of R1. Therefore, R1 might return color = ruby or color = garnet for either a consistent read or an eventually consistent read. Also, depending on the amount of time that has elapsed, an eventually consistent read might return no results.\nFor a consistent read, R2 returns color = garnet. For an eventually consistent read, R2 might return color = ruby or color = garnet depending on the amount of time that has elapsed.\nIn the last example, Client 2 performs W2 before Amazon S3 returns a success for W1, so the outcome of the final value is unknown (color = garnet or color = brick). Any subsequent reads (consistent read or eventually consistent) might return either value. Also, depending on the amount of time that has elapsed, an eventually consistent read might return no results.\nPrefix - Listing Keys Hierarchically Using a Prefix and Delimiter A \u0026ldquo;prefix\u0026rdquo;, or \u0026ldquo;key prefix\u0026rdquo; is a logical grouping of the objects in a bucket. The prefix value is similar to a directory name that enables you to store similar data under the same directory in a bucket. Note that Amazon S3 does not have the concept of \u0026ldquo;directory\u0026rdquo;. There is only objects whose keys are prefixed in a way that looks like a \u0026ldquo;directory\u0026rdquo;. For example\nzoo/west-side/fish/little-fish.png There is no such directories called zoo, west-side, fish. In this case the :ref:key \u0026lt;concept-keys\u0026gt; for the image little-fish.png is zoo/west-side/fish/, a string essentially.\nThe prefix and delimiter parameters limit the kind of results returned by a list operation. The prefix limits the results to only those keys that begin with the specified prefix. The delimiter causes a list operation to roll up all the keys that share a common prefix into a single summary list result.\nThe purpose of the prefix and delimiter parameters is to help you organize and then browse your keys hierarchically. To do this, first pick a delimiter for your bucket, such as slash (/), that doesn\u0026rsquo;t occur in any of your anticipated key names. Next, construct your key names by concatenating all containing levels of the hierarchy, separating each level with the delimiter.\nFor example, if you were storing information about cities, you might naturally organize them by continent, then by country, then by state. Because these names do not usually contain punctuation, you might select slash (/) as the delimiter. The following examples use a slash (/) delimiter.\n Europe/France/Nouvelle-Aquitaine/Bordeaux North America/Canada/Quebec/Montreal North America/USA/Washington/Bellevue North America/USA/Washington/Seattle  By using Prefix and Delimiter with the list operation, you can use the hierarchy you\u0026rsquo;ve created to list your data. For example, to list all the states in USA, set Delimiter='/' and Prefix='North America/USA/'.\nA list request with a delimiter lets you browse your hierarchy at just one level, skipping over and summarizing the (possibly millions of) keys nested at deeper levels. For example, assume you have a bucket (ExampleBucket) with the following keys.\nsample.jpg photos/2006/January/sample.jpg photos/2006/February/sample2.jpg photos/2006/February/sample3.jpg photos/2006/February/sample4.jpg The sample bucket has only the sample.jpg object at the root level. To list only the root level objects in the bucket you send a GET request on the bucket with \u0026quot;/\u0026quot; delimiter character. In response, Amazon S3 returns the sample.jpg object key because it does not contain the \u0026quot;/\u0026quot; delimiter character. All other keys contain the delimiter character. Amazon S3 groups these keys and return a single CommonPrefixes element with prefix value photos/ that is a substring from the beginning of these keys to the first occurrence of the specified delimiter.\n","date":"25","image":null,"permalink":"https://ostwind.qubitpi.org/blog/amazon-s3/","tags":null,"title":"Amazon S3 Concepts"},{"categories":null,"contents":"Jersey uses HK2 as its dependency injection (DI) system. We can use other injection systems, but its infrastructure is built with HK2, and allows us to also use it within our applications.\nSetting up simple dependency injection with Jersey takes just a few lines of code. Let say for example we have a service we would like to inject into our resources.\npublic class GreetingService { public String getGreeting(String name) { return \u0026#34;Hello \u0026#34; + name + \u0026#34;!\u0026#34;; } } And we want to inject this service into a Jersey resource\n@Path(\u0026#34;greeting\u0026#34;) public class GreetingResource { @Inject public GreetingService greetingService; @GET public String get(@QueryParam(\u0026#34;name\u0026#34;) String name) { return this.greetingService.getGreeting(name); } } In order for the injection to work, all we need is a simple configuration\n@ApplicationPath(\u0026#34;/api\u0026#34;) public class AppConfig extends ResourceConfig { public AppConfig() { register(GreetingResource.class); register(new AbstractBinder() { @Override protected void configure() { bindAsContract(GreetingService.class); } }); } } Here we are saying that we want to bind the GreetingService to the injection system, and advertise it as injectable by the same class. What the last statement means is that we can only inject it as GreetingService and (probably obviously) not by any other class. As you will see later, it is possible to change this.\n Note: The injection above is field injection, where the service is injected into the field of the resource. Another type of injection is constructor injection, where the service is injected into the constructor. Ostwind uses constructor injection ubiquitously. An example of the constructor injection is shown below:\nprivate final GreetingService greetingService; @Inject public GreetingResource(GreetingService greetingService) { this.greetingService = greetingService; } Ostwind chooses constructor injection as opposed to field injection, as it makes the resource easier to unit test. Constructor injection doesn\u0026rsquo;t require any different configuration.\n Lets say that instead of a class, the GreetingService is an interface, and we have an implementation of it (which is very common). To configure that, we would use the following syntax in the above configure method\n@Override protected void configure() { bind(NiceGreetingService.class).to(GreetingService.class); } This reads as \u0026ldquo;bind NiceGreetingService, and advertise it as GreetingService\u0026rdquo;. This means we can use the exact same code in the GreetingResource above, because we advertise the contract as GreetingService and not NiceGreetingService. But the actual implementation, when injected, will be the NiceGreetingService.\nIf you\u0026rsquo;ve ever worked with any injection framework, you will have came across the concept of scope, which determines the lifespan of the service. You may have heard of a \u0026ldquo;Request Scope\u0026rdquo;, where the service is alive only for the life of the request. Or a \u0026ldquo;Singleton Scope\u0026rdquo;, where there is only one instance of the service. We can configure these scopes also using the following syntax.\n@Override protected void configure() { bind(NiceGreetingService.class) .to(GreetingService.class) .in(RequestScoped.class); } The default scope is PerLookup, which means that every time this service is requested, a new one will be created. In the example above, using the RequestScoped, a new service will be created for a single request. This may or may not be the same as the PerLookup, depending on how many places we are trying to inject it. We may be trying to inject it into a filter and into a resource. If this were PerLookup, then two instances would be created for each request. In this case, we only want one.\nThe other two scopes available are Singleton (only one instance created) and Immediate (like Singleton) but is created on startup (whereas with Singleton, it\u0026rsquo;s not created until the first request).\nAside from binding classes, we could also just use an instance. This would gives us a default singleton, so we don\u0026rsquo;t need to use the in syntax.\n@Override protected void configure() { bind(new NiceGreetingService()) .to(GreetingService.class); } What if we have some complex creation logic or need some request context information for the service. In this case there are Factorys. For example\npublic class GreetingServiceFactory implements Factory\u0026lt;GreetingService\u0026gt; { @Context UriInfo uriInfo; @Override public GreetingService provide() { return new GreetingService( uriInfo.getQueryParameters().getFirst(\u0026#34;name\u0026#34;)); } @Override public void dispose(GreetingService service) { /* noop */ } } Here we have a factory, that gets request information from the UriInfo, in this case a query parameters, and we create the GreetingService from it. To configure it, we use the following syntax\n@Override protected void configure() { bindFactory(GreetingServiceFactory.class) .to(GreetingService.class) .in(RequestScoped.class); } ","date":"26","image":null,"permalink":"https://ostwind.qubitpi.org/development/jersey-di-using-hk2/","tags":null,"title":"Basic Dependency Injection using Jersey's HK2"},{"categories":null,"contents":"Ostwind is developed in Jersey framework.\nNOTE: In case you are not familiar with Jersey, it is a parallel technology with \u0026ldquo;Spring Boot framework\u0026rdquo;. Ostwind offers absolutely NO support for Spring and will remain as an exclusive Jersey application in the future, because Jersey, alone with its backing technology HK2, is the reference-implementation of JSR-370 (and HK2, JSR-330) standards while Spring is not.\nBy \u0026ldquo;having no support for Spring\u0026rdquo;, Ostwind means the following:\n Ostwind DOES NOT, AND WILL NOT, run as a Spring Boot Webservice Ostwind has ABSOLUTE ZERO direct-dependency from Spring Ostwind runs in NON-SPRING containers, such as Jetty  Ostwind rejects any conducts that violate the 3 rules above. NO EXCEPTION.\nOverview The following guide is intended to help developers who maintain or want to make changes to the Ostwind framework.\nBuilding Ostwind is built using Maven. Because Ostwind is a mono-repo with interdependencies between modules, it is recommended to fully build and install the project at least once:\nmvn clean install Thereafter, individual modules can be built whenever making changes to them. For example, the following command would rebuild only ostwind-core:\nmvn clean install -f ostwind-core Pull requests and release builds leverage GitHub Action. PR builds simply run the complete build along with code coverage.\nRunning Webservice in Standalone Jetty Download Jetty For JDK 17, which is the version JWT runs on, it\u0026rsquo;s been tested that Jetty 11.0.15 worked. Hence, we will use \u0026ldquo;11.0.15\u0026rdquo; release as an example:\nPut the tar.gz file into a location of your choice as the installation path and extract the Jetty binary using\ntar -xzvf jetty-home-11.0.15.tar.gz The extracted directory jetty-home-11.0.15 is the Jetty distribution. We call this directory $JETTY_HOME, which should not be modified.\nSetting Up Standalone Jetty Our WAR file will be dropped to a directory where Jetty can pick up and run. We call this directory $JETTY_BASE, which is usually different from the $JETTY_HOME. The $JETTY_BASE also contains container runtime configs. In short, the Standalone Jetty container will be setup with\nexport JETTY_HOME=/path/to/jetty-home-11.0.15 mkdir -p /path/to/jetty-base cd /path/to/jetty-base java -jar $JETTY_HOME/start.jar --add-module=annotations,server,http,deploy,servlet,webapp,resources,jsp where /path/to/ is the absolute path to the directory containing the jetty-home-11.0.15 directory\nThe --add-module=annotations,server,http,deploy,servlet,webapp,resources,jsp is how we configure the Jetty container.\nLastly, drop the WAR file into /path/to/jetty-base/webapps directory and rename the WAR file to ROOT.war:\nmv /path/to/war-file /path/to/jetty-base/webapps/ROOT.war Running Webservice java -jar $JETTY_HOME/start.jar The webservice will run on port 8080, and you will see the data you inserted\nRelease Versions Ostwind follows semantic versioning for its releases. Minor and patch versions only have the version components of MAJOR.MINOR.PATCH.\nMajor releases are often pre-released prior to the publication of the final version. Pre-releases have the format of MAJOR.MINOR.PATCH-prCANDIDATE. For example, 5.0.0-pr2 is release candidate 2 of the Ostwind 5.0.0 version.\nFAQ I Use IntelliJ. Is There Any Way to Easily Sync up with Ostwind\u0026rsquo;s Code styles For the moment, we have distilled the most important code style conventions with respect to Ostwind\u0026rsquo;s code as IntelliJ settings. If you are using IntelliJ, you may import these code style settings by importing the Ostwind-Project-intellij-code-style.xml file in the root of the repo. The setting for the project will appear as a new Scheme named Ostwind-Project under IntelliJ\u0026rsquo;s Editor -\u0026gt; Code Style section.\nAlternatively, you might check the xml file that is included in the jar and map its settings to your development environment.\nTroubleshooting Checkstyle Error - \u0026ldquo;Extra lines between braces [RegexpMultiline]\u0026rdquo; This is an Ostwind-custom checkstyle rule which simple disallow the following code snippet:\n} } Basically, multiple lines between right curly braces is defined as a checkstyle violation. The error, however, might still pops up with something visually like this:\n} } Note that no extra line can be seen in this reported case. The most probably cause might be a shared development environment where one team member wrote code on Windows, which uses CRLF line endings, and the other uses UNIX/Mac. We should replace all CRLF endings with UNIX \u0026lsquo;\\n\u0026rsquo; endings.\n","date":"26","image":null,"permalink":"https://ostwind.qubitpi.org/development/development/","tags":null,"title":"Development"},{"categories":null,"contents":"Ostwind meta store offers a GraphQL query language for file metadata API. GraphQL is a query language for APIs and a runtime for fulfilling those queries with our existing file metadata. Ostwind provides a complete and understandable description of the file data in its API, gives clients the power to ask for exactly what they need and nothing more, makes it easier to evolve APIs over time, and enables powerful developer tools.\n","date":"26","image":null,"permalink":"https://ostwind.qubitpi.org/development/metastore/","tags":null,"title":"Meta Stores - A abstraction layer for your file metadata API"},{"categories":null,"contents":"In this quickstart, we will download a OpenStack Swift Image, spinup a container on a single machine, upload a test file, and download that file.\nPrerequisites You will need:\n Docker command line tool Python 3  Getting started To download the image, issue the following commands in your terminal:\ndocker pull fnndsc/docker-swift-onlyone Setup Volume for Swift  Volumes are the preferred mechanism for persisting data generated by and used by Docker containers.\n docker volume create swift_storage Start up Container Instance docker run -d --name swift-onlyone -p 12345:8080 -v swift_storage:/srv -t fnndsc/docker-swift-onlyone Setup Swift Client Querying Swift through command line requires python-swiftclient. We\u0026rsquo;ll need to install it:\npip3 pip install python-swiftclient  If you are on Mac OS, which has Apple Swift overriding Openstack swift package, explicitly invoking swiftclient as /usr/bin/local/swift works.\nIn case the absolute path is not /usr/bin/local/swift, try sudo find / -name swift which will give you a list that contains the right \u0026gt; executable path\nFor example, /usr/local/bin/swift -A http://127.0.0.1:12345/auth/v1.0 -U chris:chris1234 -K testing stat should work. We will keep using /usr/local/bin/swift afterwards\n Create, Upload, and Download a Test File We\u0026rsquo;ve create an empty file called test-file.txt to get you started:\ntouch test-file.txt To upload this file onto Swift:\n/usr/local/bin/swift -A http://127.0.0.1:12345/auth/v1.0 -U chris:chris1234 -K testing upload --object-name test-file.txt user_uploads ./test-file.txt which will print the name of the file if the upload was successful:\ntest-file.txt To download that file:\n/usr/local/bin/swift -A http://127.0.0.1:12345/auth/v1.0 -U chris:chris1234 -K testing download user_uploads test-file.txt ","date":"26","image":null,"permalink":"https://ostwind.qubitpi.org/development/local-swift/","tags":null,"title":"Spinning Up A Local Swift Instance"},{"categories":null,"contents":"Groovy Spock We\u0026rsquo;re big believers in testing our code, both for correctness, as well as to ensure that changes don\u0026rsquo;t unintentionally break existing contracts unintentionally. For example, we rely heavily on the Spock framework for our backend service tests, and see a lot of benefit from it\u0026rsquo;s conciseness, built-in mocking framework, and the fact that it uses Groovy.\nWe also strive for very high-quality code, with the belief that quality code is easier to maintain, easier to understand, and has fewer bugs. To help keep the quality bar high. For instance we have an automated style checker (Checkstyle) in our Maven-based projects with rules that should catch most of the common style issues.\nDatabase Database-related tests contain 2 parts\n  Groovy Spock unit tests on\n Injected Query DataFetcher Injected Mutation DataFetcher    Live DB tests on endpoints\n In file servlet endpoint test and meta data servlet endpoint test, Flyway migration injects real data into a Derby in-meomroy SQL DB The Derby data is injected via a shared DBCP DataSource declared in application BinderFactory The application resource is set alive through JettyServerFactory    Reference - Apache Commons DBCP2 For testing Book example application, we use Derby\u0026rsquo;s in-memory database facility, which resides completely in main memory, not in the file system.\nIn addition, we use Apache DBCP 2 to provide DataSource pointing at the in-memory Derby instance.\nDerby Derby was meant to be used only in tests and, hence, must be imported in test scope only\n","date":"26","image":null,"permalink":"https://ostwind.qubitpi.org/development/test/","tags":null,"title":"Testing"},{"categories":null,"contents":"Configuration The configuration for Ostwind is implemented using ResourceConfig class programmatically wires up dependencies with a BinderFactory.\nThe configuration does not expose all the settings that can be customized. Some requires overriding of the injected dependency in AbstractBinderFactory, which offers the default dependency injection and resource binding.\nThe required bindings are\n  a FileStore implementation class\n  a MetaStore implementation class with dependencies of\n a QueryDataFetcher for read operation and a MutationDataFetcher for write operation    import org.apache.commons.dbcp2.BasicDataSource; import org.glassfish.hk2.utilities.binding.AbstractBinder; import graphql.schema.DataFetcher; import jakarta.inject.Provider; import javax.sql.DataSource; public class MyBinderFactory extends AbstractBinderFactory { @Override protected Class\u0026lt;? extends FileStore\u0026gt; buildFileStore() { return SwiftFileStore.class; } @Override protected Class\u0026lt;? extends MetaStore\u0026gt; buildMetaStore() { return GraphQLMetaStore.class; } @Override protected DataFetcher\u0026lt;MetaData\u0026gt; buildQueryDataFetcher() { } @Override protected DataFetcher\u0026lt;MetaData\u0026gt; buildMutationDataFetcher() { } } Any custom bindings can be achieved by overriding the afterBinding\n@Override protected void afterBinding(final AbstractBinder abstractBinder) { // Custom bindings here... } Database A database should be initialized with:\nCREATE DATABASE IF NOT EXISTS Ostwind; USE Ostwind; CREATE TABLE BOOK_META_DATA ( id int NOT NULL AUTO_INCREMENT, file_id VARCHAR(255) NOT NULL, file_name VARCHAR(255) NOT NULL, file_type VARCHAR(8) NOT NULL, PRIMARY KEY (id) ); ","date":"26","image":null,"permalink":"https://ostwind.qubitpi.org/docs/configuration/","tags":null,"title":"Configuration"},{"categories":null,"contents":"A file store is responsible for:\n  reading and writing files (.mp3, .pdf, etc.) to/from an object store. Files of the following types are supported by Ostwind\n PDF MP4 .txt file    providing \u0026ldquo;transactions\u0026rdquo; that make all file operations atomic in a single request.\n  declaring the native object store client it delegates persistence operations to.\n  If a file store implementation is unable to handle a file InputStream, Ostwind pushes these responsibilities to the object store.\nIncluded Stores Ostwind comes bundled with a number of file stores:\n Swift Store - A file store that can map operations on a file to an underlying OpenStack Swift API. Ostwind has explicit support for Swift HDFS Store - File is persisted on Hadoop HDFS.   .admonition{ border-radius: 5px; padding: 0px; border-left: 5px solid #00bcf6; box-shadow: 0 0 .5rem .2rem #00000025; } .admonition-title-container{ background-color: #00bcf6; border-top-right-radius: 5px; } .admonition-title { font-weight: bolder; font-size: large; backdrop-filter: grayscale(50%) brightness(150%); -webkit-backdrop-filter: grayscale(50%) brightness(150%); padding: 5px 0 5px 30px; border-top-right-radius: 5px; } @media (prefers-color-scheme: dark) { .admonition-title { backdrop-filter: grayscale(40%) brightness(40%); -webkit-backdrop-filter: grayscale(40%) brightness(40%); } } .admonition-content{ padding: 10px 0 10px 15px }  Tip   It is assumed that the \u0026ldquo;HDFS Store\u0026rdquo; means a single-cluster HDFS. However, the Ostwind architecture does not preclude implementing a multi-cluster HDFS store  Stores can be included through the following artifact dependencies:\nSwift Store \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;io.github.qubitpi.ostwindio.github.qubitpi.ostwind\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;ostwind-filestore-swift\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${version.ostwind}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; HDFS Store \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;io.github.qubitpi.ostwindio.github.qubitpi.ostwind\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;ostwind-filestore-hdfs\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${version.ostwind}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; Overriding the Store To change the store, override the FileStore binding. For example, to use a store called SomeCustomFileStore:\npublic class AppBinderFactory extends AbstractBinderFactory { ... @Override protected Class\u0026lt;? extends FileStore\u0026gt; buildFileStore() { return SomeCustomFileStore.class; } } Custom Stores Custom stores can be written by implementing the FileStore interface. Take Amazon S3 for instance\n@Singleton public class S3FileStore implements FileStore { private final AmazonS3 s3client; private final FileIdGenerator fileIdGenerator; @Inject public S3FileStore(@NotNull final AmazonS3 s3client, @NotNull final FileIdGenerator fileIdGenerator) { this.s3client = Objects.requireNonNull(s3client); this.fileIdGenerator = Objects.requireNonNull(fileIdGenerator); } @Override public String upload(final File file) { final String fileId = fileIdGenerator.apply(file); s3client.putObject( file.getMetaData().getFileType().name(), fileId, file.getFileContent(), new ObjectMetadata() ); return fileId; } @Override public InputStream download(final String fileId) { return getS3client().getObject(...); } } Multiple Stores A common pattern in Ostwind is the need to support multiple file stores. Typically, one file store manages most files, but some others may require a different object storage backend or have other needs to specialize the behavior of the store. The multiplex store in Ostwind manages multiple stores - delegating calls to the appropriate store which is responsible for a particular file.\nThis is a feature yet to be offered soon.\n","date":"26","image":null,"permalink":"https://ostwind.qubitpi.org/docs/filestore/","tags":null,"title":"File Stores"},{"categories":null,"contents":"The easiest way to get started with Ostwind is to use the Ostwind Book App Starter. The starter bundles all of the dependencies we will need to stand up a web service. This tutorial uses the starter, and all of the code is available here. We will deploy and play with this example locally\nDocker Compose Ostwind Compose is a tool for setting up and running a full-fledged Ostwind instance Docker application. With Compose, an Ostwind application is backed by a real MySQL meta store and an in-memory OpenStack Swift service. With a single command, we will be able to create and start all the services from Ostwind. It\u0026rsquo;s the quickest approach to get a taste of Ostwind.\nOstwind Compose works in all environments: production, staging, development, testing, as well as CI workflows. You can learn more about it from source code.\nUsing Ostwind Compose is basically a three-step process:\n Package Ostwind at project root with mvn clean package cd into compose top directory and fire-up docker compose up hit Ostwind at http://localhost:8080/v1/metadata/graphql?query={metaData(fileId:%221%22){fileName}} with your favorite browser  For more information about the Ostwind Compose the Compose file definition.\nOstwind Compose has ability for managing the whole lifecycle of an Ostwind application:\n Start, stop, and rebuild services View the status of running services Stream the log output of running services Run a one-off command on a service  Extending Ostwind Compose Happy with Ostwind? You can go further with productionizing Ostwind from here  .admonition{ border-radius: 5px; padding: 0px; border-left: 5px solid #00bcf6; box-shadow: 0 0 .5rem .2rem #00000025; } .admonition-title-container{ background-color: #00bcf6; border-top-right-radius: 5px; } .admonition-title { font-weight: bolder; font-size: large; backdrop-filter: grayscale(50%) brightness(150%); -webkit-backdrop-filter: grayscale(50%) brightness(150%); padding: 5px 0 5px 30px; border-top-right-radius: 5px; } @media (prefers-color-scheme: dark) { .admonition-title { backdrop-filter: grayscale(40%) brightness(40%); -webkit-backdrop-filter: grayscale(40%) brightness(40%); } } .admonition-content{ padding: 10px 0 10px 15px }  Business Scenario Testing   Ostwind also comes with an example acceptance testing module which can be pull out separately for our own project needs.  If you would like to go from basic Ostwind Compose setup and changed anything, rebuild it with\ndocker compose up --build --force-recreate Ostwind Compose has been tested with MySQL 5.7 connected using mysql-connector-java 5.1.38 within Ostwind running on Jetty 11.0.15.\nWarning   Please take extra caution with MySQL 8, as some of the features might not work properly on Ostwind Compose. In addition, make sure ?autoReconnect=true\u0026amp;useSSL=false is in connection string. For example, jdbc:mysql://db:3306/Ostwind?autoReconnect=true\u0026amp;useSSL=false  MySQL Container (Meta Store) Ostwind Compose uses MySQL 5 as the backing meta store, i.e. the database that DataFetcher is talking to for file metadata.\nThe MySQL instance is network-reachable at 3306 inside compose and 3305 for host (wo choose 3305 just in case 3306 has already been occupied)\nNetworking in Ostwind Compose By default Ostwind Compose sets up a single network. Both Ostwind and MySQL container services join this default network and is both reachable by other containers on that network, and discoverable by them at a hostname identical to the container name.\nFor example, inside docker-compose.yml\nservices: web: build: . ports: - \u0026#34;8080:8080\u0026#34; depends_on: db: condition: service_healthy db: image: \u0026#34;mysql:5.7\u0026#34; ports: - \u0026#34;3305:3306\u0026#34; volumes: - \u0026#34;./mysql-init.sql:/docker-entrypoint-initdb.d/mysql-init.sql\u0026#34; environment: MYSQL_ROOT_PASSWORD: root healthcheck: test: mysqladmin ping -h localhost -u root -proot timeout: 3s retries: 3 When you run docker compose up, the following happens:\n A network called \u0026ldquo;ostwind-example-books\u0026rdquo; is created. An Ostwind container is created using ostwind-example-books configuration. It joins the network \u0026ldquo;ostwind-example-books\u0026rdquo; under the name \u0026ldquo;web\u0026rdquo;. An MySQL container is created using db\u0026rsquo;s configuration. It joins the network \u0026ldquo;ostwind-example-books\u0026rdquo; under the name \u0026ldquo;db\u0026rdquo;.  Each container can now look up the hostname web or db and get back the appropriate container\u0026rsquo;s IP address. For example, web\u0026rsquo;s application code could connect to the URL mysql://db:3306 and start using the MySQL database.\nProduction Creating a Deployable War File We build the \u0026ldquo;.war\u0026rdquo; File first by navigating to ostwind project root and compile the project\ncd ostwind/ mvn clean package Successfully executing the command above shall generate a \u0026ldquo;.war\u0026rdquo; file under ostwind/ostwind-examples/ostwind-example-books/target/ostwind-example-books-\u0026lt;ostwind-version\u0026gt;.war, where is the version of the ostwind, for example 1.0.2, please make sure to replace \u0026lt;ostwind-version\u0026gt; with one of our release versions.\nSetting Up Jetty Downloading Jetty At download page, pick up a .tgz distribution, we will use \u0026ldquo;9.4.44.v20210927\u0026rdquo; release as an example:\nInstalling Jetty Put the tar.gz file into a preferred location as the installation path and extract the Jetty binary using\ntar -czvf jetty-distribution-9.4.44.v20210927.tar.gz Dropping the \u0026ldquo;.war\u0026rdquo; File into the Jetty \u0026ldquo;webapp\u0026rdquo; cd jetty-distribution-9.4.44.v20210927/webapps/ mv /path/to/.war . Then rename the war file to \u0026ldquo;ROOT.war\u0026rdquo;, the reason of which is so that the context path would be root context - /, which is a common industry standard.\nSetting a Context Path   The context path is the prefix of a URL path that is used to select the context(s) to which an incoming request is passed. Typically a URL in a Java servlet server is of the format http://hostname.com/contextPath/servletPath/pathInfo, where each of the path elements can be zero or more \u0026ldquo;/\u0026rdquo; separated elements. If there is no context path, the context is referred to as the root context. The root context must be configured as \u0026ldquo;/\u0026rdquo; but is reported as the empty string by the servlet API getContextPath() method.\nHow we set the context path depends on how we deploy the web application (or ContextHandler). In this case, we configure the context path by naming convention:\nIf a web application is deployed using the WebAppProvider of the DeploymentManager without an XML IoC file, then the name of the WAR file is used to set the context path:\n If the WAR file is named \u0026ldquo;myapp.war\u0026rdquo;, then the context will be deployed with a context path of /myapp If the WAR file is named \u0026ldquo;ROOT.WAR\u0026rdquo; (or any case insensitive variation), then the context will be deployed with a context path of / If the WAR file is named \u0026ldquo;ROOT-foobar.war\u0026rdquo; (or any case insensitive variation), then the context will be deployed with a context path of / and a virtual host of \u0026ldquo;foobar\u0026rdquo;    Starting the Webservice cd ../ java -jar start.jar  Tip   To specify the port that container exposes for our app, we could use\njava -jar start.jar -Djetty.port=8081   Firing The First Request brew install --cask graphiql ","date":"26","image":null,"permalink":"https://ostwind.qubitpi.org/docs/intro/","tags":null,"title":"Getting Started"},{"categories":null,"contents":"This is collection of terms related to Ostwind and its concepts.\nApplication Concerns Health Check A Health Check is a mechanism to programatically assert if the web service is healthy or not in a binary yes/no fashion.\nFeature Flag A Feature Flag is a boolean configuration mechanism that can be used to turn certain capabilities on or off via a simple flag-like setting.\nSystem Config System Config is a layered configuration infrastructure that makes it easy to handle configuration within the code, as well as easy to specify configuration in different environments.\nRequest Log The Request Log is an extensible log line that Ostwind emits after a request has been handled and responded to. The data in this log line is built up as the request is processed and it includes information about nearly every phase of processing a request, including how long things took at both fine-grained and aggregate levels.\nFile Store A File Store is the generic name for the source of the users' files, like OpenStack Swift and Hadoop Distributed File System (HDFS)\nMiscellaneous Spock Spock is a Groovy-based BDD-style testing framework.\nGroovy Groovy is a dynamic JVM-based programming language. It\u0026rsquo;s dynamic and flexible nature make it particularly good for uses like testing.\nServlet A Servlet is a Java construct that usually is designed to handle an HTTP request. For Ostwind, we also have a Servlet construct, and while it\u0026rsquo;s similar to the Java construct, it\u0026rsquo;s more akin to a Controller in other MVC web frameworks like Ruby on Rails or Grails.\nMeta Data A Meta Data is some piece of combined information that describes a general file, such as file name and file type.\nWeb Service A software system, usually located at the server side in a client-server organization on the web, acting as middleware or interface between a client and a database server. In a more general definition, W3C defines web service as a software system designed to support interoperable machine-to-machine interaction over a network.\nSee also web service.\n","date":"26","image":null,"permalink":"https://ostwind.qubitpi.org/docs/glossary/","tags":null,"title":"Glossary"},{"categories":null,"contents":"Graph APIs are an evolution of web service APIs that serve and manipulate data for mobile \u0026amp; web applications. They have a number of characteristics that make them well suited to this task:\n  Most notably, they present a data model as an entity relationship graph and an accompanying schema.\n A well-defined model allows for a consistent view of the data and a centralized way to manipulate an instance of the model or to cache it. The schema provides powerful introspection capabilities that can be used to build tools to help developers understand and navigate the model    The API allows the client to fetch or mutate as much or as little information in single roundtrip between client and server. This also shrinks payload sizes and simplifies the process of schema evolution\n  There is a well-defined standard for the API that fosters a community approach to development of supporting tools \u0026amp; best practices.\n  ","date":"26","image":null,"permalink":"https://ostwind.qubitpi.org/docs/graphql/","tags":null,"title":"GraphQL"},{"categories":null,"contents":"JSON-API is a specification for building REST APIs for CRUID (create, read, update, and delete) operations.\nSimilar to GraphQL, it allows the client to control what is returned in the response payload. Unlike GraphQL, the JSON-API spells out hot to perform file operations instead of file metadata operations.\nJSON-API has no standardized schema introspection. However, Ostwind adds this capability to file service by exporting an Open API Initiative document (formerly known as Swagger). The json-api specification is the best reference for understanding JSON-API. The following sections describe commonly used JSON-API features as well as Ostwind additions for filtering and swagger.\nHierarchical URLs Ostwind generally follows the JSON-API recommendations for URL design.\nOstwind currently requires all files to be addressed by ID within a URL parameter. For example, downloading a file with an ID of 1 must be fully qualified by ID: /file/download?fileId=1\nFile Identifiers Ostwind supports two mechanisms by which a newly uploaded file is assigned an ID:\n The file is assigned by a fixed-length ID and saved in the data store. The application provides an ID which is generated by a custom FileIdGenerator implementation by the application   .admonition{ border-radius: 5px; padding: 0px; border-left: 5px solid #00bcf6; box-shadow: 0 0 .5rem .2rem #00000025; } .admonition-title-container{ background-color: #00bcf6; border-top-right-radius: 5px; } .admonition-title { font-weight: bolder; font-size: large; backdrop-filter: grayscale(50%) brightness(150%); -webkit-backdrop-filter: grayscale(50%) brightness(150%); padding: 5px 0 5px 30px; border-top-right-radius: 5px; } @media (prefers-color-scheme: dark) { .admonition-title { backdrop-filter: grayscale(40%) brightness(40%); -webkit-backdrop-filter: grayscale(40%) brightness(40%); } } .admonition-content{ padding: 10px 0 10px 15px }  Tip   In order to replace the [default file ID generator][FileNameAndUploadedTimeBasedIdGenerator], the custom implementation must be bound explicitly through [BinderFactory][AbstractBinderFactory]. For example, when application implements a generator called \u0026lsquo;MyIdGenerator\u0026rsquo;, it will load via\n@Override protected Class\u0026lt;? extends FileIdGenerator\u0026gt; buildFileIdGenerator() { return MyIdGenerator.class } For more info on custom binding, please check out Basic Dependency Injection using Jersey\u0026rsquo;s HK2\n  ","date":"26","image":null,"permalink":"https://ostwind.qubitpi.org/docs/json-api/","tags":null,"title":"JSON API"},{"categories":null,"contents":"These are the key performance indicators for the Ostwind Web Service component, listed in categories by order of importance.\nServer Error Responses (HTTP 5XX) Shows how much trouble the service is having.\n com.codahale.metrics.servlet.AbstractInstrumentedFilter.responseCodes.serverError.m1_rate  Swift Errors Shows how much trouble queries are having against swift.\n swift.errors.exceptions.m1_rate swift.errors.http.m1_rate  Requests Shows how many requests the service is serving.\n com.codahale.metrics.servlet.AbstractInstrumentedFilter.requests.m1_rate com.codahale.metrics.servlet.AbstractInstrumentedFilter.requests.m15_rate  System Metrics Shows the overall health of the system\u0026rsquo;s low-level resources and activities.\n CPU Memory Network IO GC Pauses  Latency Shows duration of overall requests and druid requests. (m1_rate and pN)\n com.codahale.metrics.servlet.AbstractInstrumentedFilter.requests.p50 com.codahale.metrics.servlet.AbstractInstrumentedFilter.requests.p75 com.codahale.metrics.servlet.AbstractInstrumentedFilter.requests.p95 com.codahale.metrics.servlet.AbstractInstrumentedFilter.requests.p98 com.codahale.metrics.servlet.AbstractInstrumentedFilter.requests.p99 com.codahale.metrics.servlet.AbstractInstrumentedFilter.requests.p999  Rate Limiting Rejections Shows if users are hitting rate limits.\n ratelimit.meter.reject.ui.m1_rate ratelimit.meter.reject.user.m1_rate ratelimit.meter.reject.global.m1_rate  Active Requests Shows load at a given point in time. (ie. how close are the load is to the limits of Swift)\n com.codahale.metrics.servlet.AbstractInstrumentedFilter.activeRequests.count  Bad Request Responses (HTTP 4XX) Shows how much trouble users are having interacting with the API.\n com.codahale.metrics.servlet.AbstractInstrumentedFilter.responseCodes.badRequest.m1_rate com.codahale.metrics.servlet.AbstractInstrumentedFilter.responseCodes.notFound.m1_rate  ","date":"26","image":null,"permalink":"https://ostwind.qubitpi.org/docs/kpi/","tags":null,"title":"Key Performance Indicators - Ostwind Web Service"},{"categories":null,"contents":"Logs are an important tool for debugging problems, as well as for gaining insight into how something is running under otherwise normal conditions. Because logs are so useful, it\u0026rsquo;s tempting to always log everything that\u0026rsquo;s going on, but processing logs is much more work than it would seem and writing too many logs has a significant negative impact on performance.\nThe Ostwind logs are Jetty logs, so they can be found wherever your Jetty instance stores its logs.\nSummary Details and examples are below, but here\u0026rsquo;s a summary of the log levels and their meanings.\n   Level Meaning     Error System-caused problem preventing correct results for requests. Major, \u0026ldquo;wake up a human\u0026rdquo; events.   Warn Something\u0026rsquo;s wrong, like no caching, but can still correctly respond to requests. A human should investigate.   Info Working as expected. Information needed for monitoring. Answers \u0026ldquo;Are things healthy.\u0026rdquo;   Debug High-level data flow and errors. Rich insight, but not overwhelming. Per-request errors, but not happy-path.   Trace Most verbose. Full data flow. Not intended to be used in production, or very rarely.    Log Levels To be able to balance the trade off between logging more information and having better performance, Ostwind\u0026rsquo;s logging framework allows controlling which log messages are processed using Log Levels. Here are some guidelines to help figure out what level a particular message should be logged at. By following these guidelines, logging will be handled consistently across the Ostwind codebase.\nError Should be used when Ostwind is having trouble and cannot reliably respond to requests. Under normal running conditions, there shouldn\u0026rsquo;t be Error-level logs emitted by Ostwind. Any Error-level logs that are emitted by Ostwind should be major events that likely require urgent intervention. Here are some examples:\n  There is a configuration error that will prevent the application from working correctly.\n  There is an unexpected error (ie. not caused by the user) that is going to result in failing the request with a 500 HTTP status code.\nExpected request failure conditions that are not the user\u0026rsquo;s fault which result in 5xx-level HTTP status codes should not result in a log at the Error level. Examples of these include 503 Service Unavailable responses when some runtime data/objects hasn\u0026rsquo;t been loaded, or 507 Insufficient Storage when a request exceeds the weight limit.\n  Warn Should be used when something isn\u0026rsquo;t right, but Ostwind can still reliably respond to requests. Under normal running conditions, there may be Warn-level logs emitted by Ostwind. Warn-level logs that are emitted by Ostwind should be looked at by a human, but should not typically require urgent intervention. Here are some examples:\n  There is a configuration error that will not prevent requests from being correctly satisfied, but requests may be satisfied non-optimally or with some other down-side.\n  The app tries to start as indicated, but has to fall back to some default configuration because an error was encountered. The app can still start, but not the way it expected to.\n  A non-critical service is unavailable or is having trouble. It\u0026rsquo;s possible that the issue will correct itself, but that depends on the service.\nFor example, the Swift response cache might be having errors or timing out. This won\u0026rsquo;t prevent Ostwind from correctly responding to the request, but it may not be as fast due to the cache not working.\n  Info Should be used to indicate status and telemetry for each HTTP request processed, as well as non-recurring events and information about the state of Ostwind. It is expected that Ostwind will be running at the Info logging level when running in production. As such, the information logged at this level should be the information needed for monitoring purposes, meaning that it effectively answers \u0026ldquo;Is the application healthy?\u0026rdquo; and \u0026ldquo;How is the application being used?\u0026rdquo;\n Startup / shutdown information (success or failure of main components starting or stopping) Timing and telemetry about processing each request  Debug Should be used to indicate high-level data flow and request handling steps. It\u0026rsquo;s expected that the Debug log level will be used when determining why something is behaving unexpectedly. As such, the information logged at the Debug level should give a fairly rich insight into what is happening, but should not contain so much information as to be overwhelming.\nOne exception to this is that \u0026ldquo;normal\u0026rdquo; or \u0026ldquo;happy path\u0026rdquo; request processing flow should not be logged at the Debug level. This is because it\u0026rsquo;s assumed that the bulk of the work will be for processing requests and we don\u0026rsquo;t want to flood the log with too many \u0026ldquo;everything is working as normal\u0026rdquo; events. Here are some examples of what should be logged at the Debug level:\n When the request is going to fail due to user error, like an improper date format or other input validation error. Once-per-request information, like which Swift Account/Container is selected. Normal execution flow while starting up / shutting down. Normal execution flow while running, but not processing a request.  Trace Trace is the most verbose log level and it\u0026rsquo;s expected that it will only be turned on very rarely in a production setting. This is the level at which to log \u0026ldquo;normal\u0026rdquo; execution flow for request handling.\n Normal execution flow while processing a request. Large or tight loops for any kind of execution (app-start, non-request, request processing). If the loop is time-sensitive or particularly hot, however, consider logging outside the loop so as not to impede performance.  ","date":"26","image":null,"permalink":"https://ostwind.qubitpi.org/docs/logging-guidelines/","tags":null,"title":"Logging Guidelines"},{"categories":null,"contents":"Ostwind was built from the start to be easy to operate and debug. There are 3 primary ways in which Ostwind makes debugging and gathering information about how it\u0026rsquo;s running easy.\nLogs Ostwind has strict guidelines around what information should be logged, when it should be logged, and what level it should be logged at. Here\u0026rsquo;s a brief summary of the log levels and what they contain:\n   Level Meaning     Error System-caused problem preventing correct results for requests. Major, \u0026ldquo;wake up a human\u0026rdquo; events.   Warn Something\u0026rsquo;s wrong, like no caching, but can still correctly respond to requests. A human should investigate.   Info Working as expected. Information needed for monitoring. Answers \u0026ldquo;Are things healthy.\u0026rdquo;   Debug High-level data flow and errors. Rich insight, but not overwhelming. Per-request errors, but not happy-path.   Trace Most verbose. Full data flow. Not intended to be used in production, or very rarely.    Request Log Ostwind also has a tracing mechanism that records and collects information about each request as it flows through the system. This information is called the Request Log and it gets logged at the INFO level when the response for a request is sent.\nThe Request Log is modular, and has a number of different components, depending on which type of request is being served. All Request Logs contain a Durations, Threads, and Preface component at the beginning, an Epilogue component at the end, and other components that are added as requests are processed will get added in between in the order they are added. If desired, this ordering can be controlled by setting the requestlog_loginfo_order property.\nAs part of the Request Log tracing, each request is assigned a UUID that which shows up in the INFO line for Request Log log lines. This UUID is also made available on every log line via MDC under the key logid so that all log lines emitted while processing a request can be easily collected together. To surface this UUID in all log lines emitted while processing a request, your log format needs to include a reference to the UUID value in MDC. Here are some examples for common logging frameworks:\nLogback\n%mdc{logid:-unset} %X{logid:-unset} Note that the -unset sets the default value to unset if there is no logid value present in MDC, which is possible for logs emitted outside of processing a request, like background processes.\nLog4J\n%X{logid} Log4J2\n%X{logid} %mdc{logid} %MDC{logid} Metrics Ostwind uses the Metrics library (formerly Dropwizard Metrics) for gathering and reporting on runtime metrics and indicators. Typically, these are exposed through an admin servlet, and if that\u0026rsquo;s enabled then there is a list of Key Performance Indicators (KPIs) that are exposed through the /metrics endpoint. The KPI document doesn\u0026rsquo;t go into detail about what each of the KPIs mean, but it gives a rough overview of why they matter.\nHealth Checks Ostwind also uses the Metrics library for implementing health checks. These are also exposed through the admin servlet, just like metrics, at /status. Each of the health checks has a message and a status, and if any of the checks fail, the HTTP Status Code for that /status request will be a 500 instead of a 200 to indicate the system is unhealthy.\nThese health checks are also used to gate /file and /metadata requests, with Ostwind returning 503 Service Unavailable if it doesn\u0026rsquo;t think it is healthy.\n","date":"26","image":null,"permalink":"https://ostwind.qubitpi.org/docs/monitoring-and-operations/","tags":null,"title":"Monitoring and Operations"},{"categories":null,"contents":"Ostwind has two main configuration avenues, the domain object configuration (File Store, Meta Store, and Data Fetchers) which happens via compiled Java code, and system configuration via properties. The domain configuration is covered elsewhere, and we\u0026rsquo;ll only cover the system configuration infrastructure here.\nThe system for property configuration that Ostwind uses lives in it\u0026rsquo;s own sub-module. This system is extensible and reusable so that other Ostwind modules, and even other projects, can leverage it for their own property config needs. That sub-module has it\u0026rsquo;s own deep set of documentation, so we\u0026rsquo;ll be focusing only on how to use it for configuring Ostwind.\nConfiguration Sources and Overrides Configuration for Ostwind modules come from only one location (that is, within the sub-module itself) and allows for overriding other settings. This is particularly useful when overriding a property set in a module to turn off a feature, or to override a default configuration for your application in a certain environment, for example.\nConfiguration sources are shown below, and are resolved in priority order, with higher-priority sources overriding settings from lower-priority sources. Sources that are files will available to Ostwind on the Classpath for them to be loaded.\n   Priority Source Notes     (High) 1 Environment variables    2 Java properties    3 userConfig.properties    5 applicationConfig.properties Every application MUST provide one of these     .admonition{ border-radius: 5px; padding: 0px; border-left: 5px solid #00bcf6; box-shadow: 0 0 .5rem .2rem #00000025; } .admonition-title-container{ background-color: #00bcf6; border-top-right-radius: 5px; } .admonition-title { font-weight: bolder; font-size: large; backdrop-filter: grayscale(50%) brightness(150%); -webkit-backdrop-filter: grayscale(50%) brightness(150%); padding: 5px 0 5px 30px; border-top-right-radius: 5px; } @media (prefers-color-scheme: dark) { .admonition-title { backdrop-filter: grayscale(40%) brightness(40%); -webkit-backdrop-filter: grayscale(40%) brightness(40%); } } .admonition-content{ padding: 10px 0 10px 15px }  Tip   Since userConfig.properties is often used while developing to turn features on and off, .gitignore includes a rule to ignore this file by default to help prevent checking it in accidentally.  ","date":"26","image":null,"permalink":"https://ostwind.qubitpi.org/docs/system-config/","tags":null,"title":"System Configuration"},{"categories":null,"contents":"Yahoo stores more than 250 Billion objects and half an exabyte of perpetually durable user content such as photos, videos, email, and blog posts. Object storage at Yahoo is growing at 20-25% annually. The growth is primarily driven by mobile, images, video, and user growth. Yahoo is betting on software defined storage to scale storage cost effectively along with the durability and latency guarantees.\nObject Storage Landscape at Yahoo What is \u0026ldquo;object storage\u0026rdquo;? Images and photos in Flickr, Videos, and documents, spreadsheets, and presentations exchanged as Mail attachments are classic examples of \u0026ldquo;objects\u0026rdquo;. The typical quality of this class of data is \u0026ldquo;write-once-read-many\u0026rdquo;. Traditionally, Yahoo has used storage appliances for object storage. As Yahoo is increasingly becoming the guide for digital information to our users, object storage need in Yahoo is growing rapidly. Additionally, application characteristics differ in access patterns, durability and latency needs, and cost targets. To support growth cost effectively and meet the varying application needs, object storage in Yahoo requires different tradeoffs. We need the flexibility offered by software defined storage to deliver these tradeoffs.\nWhy Software Defined Storage Key benefits of software defined storage are:\n Cost-performance tradeoff: Allows applications to choose performance and cost tradeoffs with different hardware and durability configurations using the same software stack. Flexible interfaces: Ability to choose industry standard API, embed client libraries in applications, or even use proprietary API where required. Industry standard APIs allow seamless migration of applications from public to Yahoo private cloud. Different storage abstractions: Leverage the same storage software stack across Object, Block, and File abstractions, thus reducing R\u0026amp;D and operational costs.  Cloud Object Store (COS) is Yahoo\u0026rsquo;s commodity hardware based software defined storage solution. In partnership with Flickr Yahoo has completed a multi-petabyte initial deployment of COS. Yahoo plans COS as a multi-tenant hosted service and to grow COS by ten-fold to support Flickr, Yahoo Mail and Tumblr. That is 100s of petabytes of storage to be supported on COS.\nUnder the Hood COS is deployed using Ceph storage technology. Yahoo evaluated open-source solutions such as Swift and Ceph, as well as commercial solutions and chose Ceph because it enables consolidation of storage tiers for Object, Block, and File with inherent architectural support. Also, being an open-source product, Ceph provides the flexibility needed to customize for Yahoo needs.\nCOS deployment consists of modular Ceph clusters with each Ceph cluster treated as a pod. Multiple such Ceph clusters deployed simultaneously form a COS supercluster as shown in figure below. Objects are uniformly distributed across all the clusters in a supercluster. A proprietary hashing mechanism is used to distribute objects. The hashing algorithm is implemented in a client library embedded in the applications.\nSince each cluster consists of tens of commodity servers and hundreds of disks, it is highly likely that components will fail frequently. High disk and network activity occurs during recovery due to rebalancing of objects, which in turn increases object read latency during this phase. Capping the size of each cluster allows Yahoo to limit the resource usage during recovery phases in order to adhere to latency SLAs.\nYahoo users expect their images, videos and mail attachments to be perpetually stored, and made available instantaneously from anywhere around the world. This requires high data durability guarantees. Durability is typically achieved in storage systems either via redundancy or encoding. Redundancy can be provided through extra copies of data or replicas. On the other hand, encoding can be provided via traditional mechanisms like simple parity, or more sophisticated mechanisms like erasure coding. Erasure coding breaks down an object into fragments and stores them across multiple disks with a few redundant pieces to tolerate multiple failures.\nThe usable capacity of each cluster depends on the durability technique used. We currently employ erasure coding with each object broken down into eight data and three coding fragments. This mechanism, called 8/3 erasure coding, can tolerate up to three simultaneous server and/or disk failures with about 30% storage overhead for durability. This is much lower than the 200% overhead in case of replication.\nThe two durability techniques offer different price points and latency characteristics. Replication offers lower latency but a higher cost, whereas erasure coding reduces cost (sometimes by up to 50%) at a slightly higher latency. We can also deploy different storage media such as SSD, HDD and Shingled Magnetic Recording (SMR) drives to enable different service levels depending on the application.\nTechnically, it is possible to scale a COS supercluster by adding storage needs to increase the capacity of the component clusters. However, this will lead to rebalancing of data within the component clusters, thereby creating prolonged disk and network activity and impact latency SLA. To scale COS, our preferred approach is to add COS superclusters as needed similar to adding storage farms. This approach is consistent with our current appliance-based storage solution that applications are already familiar with.\nLatency Optimizations COS is in the serving path for many Yahoo applications and has to guarantee latency SLAs to ensure consistent high quality of user experience. We have implemented over 40 optimizations in Ceph to realize 50% improvement on average, and 70% improvement in 99.99% latency. The figure below depicts the latency chart before and after the optimizations under normal operations. The latencies in this chart are measured across objects of different sizes in the Flickr workload.\nSome of the major optimizations are:\n Redundant parallel reads with erasure coding: Currently, we have deployed 8/3 erasure coding scheme for durability. Increasing the parallel reads to 11 chunks, instead of the default 8 employed in Ceph, and reconstructing the object upon first 8 retrievals provided significant improvement in long tail read latency. This reduced average latency by approximately 40%. Recovery Throttling: Upon disk and node failures, Ceph automatically initiates recovery to maintain high durability of objects. During recovery, storage nodes are busy leading to high read/write latency. We implemented tunable recovery throttle rate to mitigate this impact. This reduce average latency during recovery by approximately 60%. Bucket Sharding: Amazon S3 API specification requires objects to be bucketized. Ceph implements bucket as an object hosted on a single storage node. At our scale, the storage node that hosts the bucket becomes a hotspot, which we mitigated by implementing sharded buckets that are spread across multiple nodes.  Future Development So far, Yahoo has tuned COS to a large Yahoo use-case, namely Flickr. However, other Yahoo use cases require object storage with different workload patterns and different tradeoffs. To make COS a widely used platform at Yahoo, several enhancements in near to mid-term are\n Scale: Yahoo has already deployed an initial multi-petabyte solution planned to grow this 10-fold or more to accommodate other use cases such as Mail, Video, Tumblr etc. along with Flickr growth. Geo Replication for Business Continuity: Currently, geo replication is carried out at the application level. Ceph supports Geo-replication. However, Yahoo has not tested this capability for the scale and latency that Yahoo needs and planned to scale and deploy geo-replication in COS. Optimize latency for small objects: Many use-cases such as serving thumbnails and serving during image search have small objects of the order of a few kilobytes. COS needs to be tunned for these use-cases. Lifecycle management: One of the big advantages of Software Defined Storage is the hardware, software choices for cost and performance tradeoffs. Automatic classification of objects into hot, warm, and cold objects will allow us to take advantage of that flexibility and provide differentiated services.  ","date":"26","image":null,"permalink":"https://ostwind.qubitpi.org/blog/yahoo-object-storage/","tags":null,"title":"Yahoo Cloud Object Store - Object Storage at Exabyte Scale"},{"categories":null,"contents":"Downloads All recent supported releases may be downloaded from Maven Central: Download a release now!\nNews   26 June 2025: beta release available   Complete change log for this release     ","date":"13","image":null,"permalink":"https://ostwind.qubitpi.org/general/downloads/","tags":null,"title":"Downloads"}]