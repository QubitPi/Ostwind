<!doctype html><html><!doctype html>
<html>
<head>
<meta charset=utf-8>
<meta content="IE=edge" http-equiv=x-ua-compatible>
<meta content="width=device-width,initial-scale=1" name=viewport>
<meta content name=description>
<meta content name=author>
<title>Yahoo Cloud Object Store - Object Storage at Exabyte Scale</title>
<link href=/favicon.ico rel=icon>
<link href=https://ostwind.qubitpi.org/css/font-awesome.all.min.css rel=stylesheet>
<link href=https://ostwind.qubitpi.org/css/bootstrap.min.css rel=stylesheet>
<link href=https://ostwind.qubitpi.org/css/termynal.css rel=stylesheet>
<link href=https://ostwind.qubitpi.org/css/ostwind-theme.css rel=stylesheet>
<meta content="#da532c" name=msapplication-TileColor>
<meta content="#ffffff" name=theme-color>
</head>
<body>
<body>
<header>
<menu style=background:#000;margin:0>
<nav class="navbar navbar-expand-lg navbar-dark bg-black">
<div class=container-fluid>
<a href=https://ostwind.qubitpi.org> <img alt=Ostwind height=35 src=https://ostwind.qubitpi.org/images/logo.svg width=60></a>
<a class="header-text navbar-brand" href=https://ostwind.qubitpi.org>Ostwind</a>
<button aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation" class=navbar-toggler data-bs-target=#navbarSupportedContent data-bs-toggle=collapse type=button>
<span class=navbar-toggler-icon></span>
</button>
<div class="collapse navbar-collapse" id=navbarSupportedContent>
<ul class="navbar-nav me-auto mb-2 mb-lg-0">
<li class="nav-item dropdown">
<a aria-expanded=false class="nav-link dropdown-toggle" data-bs-toggle=dropdown href=/Document id=navbarDropdown role=button>
Documentation
</a>
<ul aria-labelledby=navbarDropdown class=dropdown-menu>
<li><a class=dropdown-item href=/docs/intro>Getting Started</a></li>
<li><a class=dropdown-item href=/docs/json-api>JSON API</a></li>
<li><a class=dropdown-item href=/docs/graphql>GraphQL</a></li>
<li><a class=dropdown-item href=/docs/filestore>File Stores</a></li>
<li><a class=dropdown-item href=/docs/configuration>Configuration</a></li>
<li><a class=dropdown-item href=/docs/system-config>System Configuration</a></li>
<li><a class=dropdown-item href=/docs/monitoring-and-operations>Monitoring and Operations</a></li>
<li><a class=dropdown-item href=/docs/logging>Logging Guidelines</a></li>
<li><a class=dropdown-item href=/docs/kpi>Key Performance Indicators</a></li>
<li>
<hr class=dropdown-divider>
</li>
<li><a class=dropdown-item href=/docs/glossary>Glossary</a></li>
</ul>
</li>
<li class="nav-item dropdown">
<a aria-expanded=false class=nav-link href=https://ostwind.qubitpi.org/apidocs/ id=navbarDropdown role=button>
API
</a>
</li>
<li class="nav-item dropdown">
<a aria-expanded=false class=nav-link href=/general/downloads id=navbarDropdown role=button>
Releases
</a>
</li>
<li class="nav-item dropdown">
<a aria-expanded=false class="nav-link dropdown-toggle" data-bs-toggle=dropdown href=# id=navbarDropdown role=button>
Contributor Guide
</a>
<ul aria-labelledby=navbarDropdown class=dropdown-menu>
<li><a class=dropdown-item href=https://ostwind.qubitpi.org/development/intro/>Getting Started</a></li>
<li><a class=dropdown-item href=https://ostwind.qubitpi.org/development/di/>Dependency Injection</a></li>
<li><a class=dropdown-item href=https://ostwind.qubitpi.org/development/test/>Testing</a></li>
<li><a class=dropdown-item href=https://ostwind.qubitpi.org/development/metastore/>Meta Stores</a></li>
<li><a class=dropdown-item href=https://ostwind.qubitpi.org/development/local-swift/>Swift (Local)</a></li>
</ul>
</li>
<li class="nav-item dropdown">
<a aria-expanded=false class="nav-link dropdown-toggle" data-bs-toggle=dropdown href=# id=navbarDropdown role=button>
Blogs
</a>
<ul aria-labelledby=navbarDropdown class=dropdown-menu>
<li><a class=dropdown-item href=https://ostwind.qubitpi.org/blog/yahoo-object-storage/>Yahoo
Cloud Object Store</a></li>
</ul>
</li>
<li>
<form action=/search class=search-bar method=get>
<input class=search-input id=search-query name=q placeholder=Search... type=search>
<button class=search-button type=submit>Search</button>
</form>
</li>
</ul>
</div>
</div>
</nav>
</menu>
</header>
<div class=content>
<div class=docs>
<p>Yahoo stores more than 250 Billion objects and half an exabyte of perpetually durable user content such as photos,
videos, email, and blog posts. Object storage at Yahoo is growing at 20-25% annually. The growth is primarily driven by
mobile, images, video, and user growth. Yahoo is betting on software defined storage to scale storage cost effectively
along with the durability and latency guarantees.</p>
<h2 id=object-storage-landscape-at-yahoo>Object Storage Landscape at Yahoo</h2>
<p>What is &ldquo;object storage&rdquo;? Images and photos in Flickr, Videos, and documents, spreadsheets, and presentations exchanged
as Mail attachments are classic examples of &ldquo;objects&rdquo;. The typical quality of this class of data is
&ldquo;<strong>write-once-read-many</strong>&rdquo;. Traditionally, Yahoo has used storage appliances for object storage. As Yahoo is
increasingly becoming the guide for digital information to our users, object storage need in Yahoo is growing rapidly.
Additionally, application characteristics differ in access patterns, durability and latency needs, and cost targets. To
support growth cost effectively and meet the varying application needs, object storage in Yahoo requires different
tradeoffs. <strong>We need the flexibility offered by software defined storage to deliver these tradeoffs</strong>.</p>
<h2 id=why-software-defined-storage>Why Software Defined Storage</h2>
<p>Key benefits of software defined storage are:</p>
<ul>
<li><strong>Cost-performance tradeoff</strong>: Allows applications to choose performance and cost tradeoffs with different hardware
and durability configurations using the same software stack.</li>
<li><strong>Flexible interfaces</strong>: Ability to choose industry standard API, embed client libraries in applications, or even use
proprietary API where required. Industry standard APIs allow seamless migration of applications from public to Yahoo
private cloud.</li>
<li><strong>Different storage abstractions</strong>: Leverage the same storage software stack across Object, Block, and File
abstractions, thus reducing R&D and operational costs.</li>
</ul>
<p>Cloud Object Store (COS) is Yahoo&rsquo;s commodity hardware based software defined storage solution. In partnership with
Flickr Yahoo has completed a multi-petabyte initial deployment of COS. Yahoo plans COS as a multi-tenant hosted service
and to grow COS by ten-fold to support Flickr, Yahoo Mail and Tumblr. That is 100s of petabytes of storage to be
supported on COS.</p>
<h2 id=under-the-hood>Under the Hood</h2>
<p>COS is deployed using <strong>Ceph</strong> storage technology. Yahoo evaluated open-source solutions such as Swift and Ceph, as well
as commercial solutions and chose Ceph because it enables consolidation of storage tiers for Object, Block, and File
with inherent architectural support. Also, being an open-source product, Ceph provides the flexibility needed to
customize for Yahoo needs.</p>
<p>COS deployment consists of modular Ceph clusters with <em>each Ceph cluster treated as a pod</em>. Multiple such Ceph clusters
deployed simultaneously form a COS <strong>supercluster</strong> as shown in figure below. Objects are uniformly distributed across
all the clusters in a supercluster. A <em>proprietary hashing mechanism</em> is used to distribute objects. The hashing
algorithm is implemented in a client library embedded in the applications.</p>
<p><img src=../../images/cepu-cluster.png alt="Error loading cepu-cluster.png"></p>
<p>Since each cluster consists of tens of commodity servers and hundreds of disks, it is highly likely that components will
fail frequently. High disk and network activity occurs during recovery due to rebalancing of objects, which in turn
increases object read latency during this phase. Capping the size of each cluster allows Yahoo to limit the resource
usage during recovery phases in order to adhere to latency SLAs.</p>
<p>Yahoo users expect their images, videos and mail attachments to be perpetually stored, and made available
instantaneously from anywhere around the world. This requires high data “durability” guarantees. Durability is typically
achieved in storage systems either via redundancy or encoding. Redundancy can be provided through extra copies of data
or replicas. On the other hand, encoding can be provided via traditional mechanisms like simple parity, or more
sophisticated mechanisms like erasure coding. Erasure coding breaks down an object into fragments and stores them across
multiple disks with a few redundant pieces to tolerate multiple failures.</p>
<p><strong>The usable capacity of each cluster depends on the durability technique used. We currently employ erasure coding with
each object broken down into eight data and three coding fragments. This mechanism, called 8/3 erasure coding, can
tolerate up to three simultaneous server and/or disk failures with about 30% storage overhead for durability. This is
much lower than the 200% overhead in case of replication</strong>.</p>
<p>The two durability techniques offer different price points and latency characteristics. Replication offers lower latency
but a higher cost, whereas erasure coding reduces cost (sometimes by up to 50%) at a slightly higher latency. We can
also deploy different storage media such as SSD, HDD and Shingled Magnetic Recording (SMR) drives to enable different
service levels depending on the application.</p>
<p>Technically, it is possible to scale a COS supercluster by adding storage needs to increase the capacity of the
component clusters. However, this will lead to rebalancing of data within the component clusters, thereby creating
prolonged disk and network activity and impact latency SLA. To scale COS, our preferred approach is to add COS
superclusters as needed similar to adding storage farms. This approach is consistent with our current appliance-based
storage solution that applications are already familiar with.</p>
<h2 id=latency-optimizations>Latency Optimizations</h2>
<p>COS is in the serving path for many Yahoo applications and has to guarantee latency SLAs to ensure consistent high
quality of user experience. We have implemented over 40 optimizations in Ceph to realize 50% improvement on average, and
70% improvement in 99.99% latency. The figure below depicts the latency chart before and after the optimizations under
normal operations. The latencies in this chart are measured across objects of different sizes in the Flickr workload.</p>
<p><img src=../../images/ceph-optimization-at-yahoo.png alt="Error loading ceph-optimization-at-yahoo.png"></p>
<p>Some of the major optimizations are:</p>
<ul>
<li><strong>Redundant parallel reads with erasure coding</strong>: Currently, we have deployed 8/3 erasure coding scheme for
durability. Increasing the parallel reads to 11 chunks, instead of the default 8 employed in Ceph, and reconstructing
the object upon first 8 retrievals provided significant improvement in long tail read latency. This reduced average
latency by approximately 40%.</li>
<li><strong>Recovery Throttling</strong>: Upon disk and node failures, Ceph automatically initiates recovery to maintain high
durability of objects. During recovery, storage nodes are busy leading to high read/write latency. We implemented
tunable recovery throttle rate to mitigate this impact. This reduce average latency during recovery by approximately
60%.</li>
<li><strong>Bucket Sharding</strong>: Amazon S3 API specification requires objects to be bucketized. Ceph implements bucket as an
object hosted on a single storage node. At our scale, the storage node that hosts the bucket becomes a hotspot, which
we mitigated by implementing sharded buckets that are spread across multiple nodes.</li>
</ul>
<h2 id=future-development>Future Development</h2>
<p>So far, Yahoo has tuned COS to a large Yahoo use-case, namely Flickr. However, other Yahoo use cases require object
storage with different workload patterns and different tradeoffs. To make COS a widely used platform at Yahoo, several
enhancements in near to mid-term are</p>
<ul>
<li><strong>Scale</strong>: Yahoo has already deployed an initial multi-petabyte solution planned to grow this 10-fold or more to
accommodate other use cases such as Mail, Video, Tumblr etc. along with Flickr growth.</li>
<li><strong>Geo Replication for Business Continuity</strong>: Currently, geo replication is carried out at the application level. Ceph
supports Geo-replication. However, Yahoo has not tested this capability for the scale and latency that Yahoo needs and
planned to scale and deploy geo-replication in COS.</li>
<li><strong>Optimize latency for small objects</strong>: Many use-cases such as serving thumbnails and serving during image search have
small objects of the order of a few kilobytes. COS needs to be tunned for these use-cases.</li>
<li><strong>Lifecycle management</strong>: One of the big advantages of Software Defined Storage is the hardware, software choices for
cost and performance tradeoffs. Automatic classification of objects into hot, warm, and cold objects will allow us to
take advantage of that flexibility and provide differentiated services.</li>
</ul>
</div>
</div>
<footer class="black-background static-bottom" style=padding:30px>
<div class=row>
<div class=col-3>
<a href=https://qubitpi.org/>
<img alt=QubitPi height=100% src=https://ostwind.qubitpi.org/images/logo.png width=100></a>
</a>
</div>
<div class=col-9>
<p class=footer-text>
My sincere thanks to <a href=https://github.com/yahoo/fili>yahoo/fili</a> &
        <a href=https://github.com/yahoo/elide>yahoo/elide</a>, which
gave tremendous amount of guidance on design and development of
<a href=https://ostwind.qubitpi.org/>Ostwind</a>, and to my former employer, Yahoo, who taught me to love
software engineering and fundamentally influenced my tech career.
</p>
</div>
</div>
<div class="copyright row">
<a href=https://ostwind.qubitpi.org style=color:grey>
The contents of this website are © 2025 Ostwind (Jiaqi Liu) under the terms of the Apache License v2.
</a>
</div>
</footer>
<script src=https://ostwind.qubitpi.org/js/bootstrap.bundle.min.js></script>
</body>
</html>